# llm-inference-js

### Requirement

Download the gemma 2b from [here](https://www.kaggle.com/models/google/gemma/tfLite/gemma-2b-it-gpu-int4)


### Getting Started

run the development server:

```bash
python -m http.server 8000
```

Open [http://localhost:8000](http://localhost:8000) with your browser to see the result.
