# llm-inference-js

### Prerequisite

A browser with WebGPU support

### Requirement

Download the gemma2 2b from [here](https://www.kaggle.com/models/google/gemma-2/tfLite/gemma2-2b-it-gpu-int8)


### Getting Started

run the development server:

```bash
python -m http.server 8000
```

Open [http://localhost:8000](http://localhost:8000) with your browser to see the result.
